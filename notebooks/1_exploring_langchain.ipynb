{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Hands-On with LangChain (2025 Edition)\n",
    "\n",
    "This notebook demonstrates LangChain's **2025 capabilities** using Azure OpenAI. We'll explore modern LCEL patterns, LangGraph agents (replacing legacy AgentExecutor), LangSmith integration, and evaluation frameworks.\n",
    "\n",
    "## üÜï What's New in 2025:\n",
    "- **LangGraph** replaces legacy AgentExecutor patterns\n",
    "- **LangSmith** production-ready monitoring and evaluation  \n",
    "- **LangChain Sandbox** for safe code execution\n",
    "- **Modern evaluation frameworks** integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's load our environment variables and configure our Azure OpenAI connection with **LangSmith tracing** for 2025 observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70d2334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded environment from: c:\\Users\\chrismckee\\Documents\\GitHub\\SemanticKernelLangChainComparison\\.env\n",
      "‚úÖ All Azure OpenAI configuration validated successfully\n",
      "Azure OpenAI Configuration:\n",
      "  Endpoint: https://aideveloperaoai.openai.azure.com\n",
      "  Deployment: gpt-4.1\n",
      "  API Version: 2025-01-01-preview\n",
      "  API Key: ********************6c6e\n",
      "  Tavily API Key: ********************2QU8\n"
     ]
    }
   ],
   "source": [
    "# Environment and configuration setup with 2025 LangSmith integration\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "class NotebookConfig:\n",
    "    \"\"\"Configuration management for Azure OpenAI and LangSmith in Jupyter notebooks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load environment variables\n",
    "        env_path = find_dotenv()\n",
    "        if env_path:\n",
    "            load_dotenv(env_path)\n",
    "            print(f\"‚úÖ Loaded environment from: {env_path}\")\n",
    "        else:\n",
    "            warnings.warn(\"No .env file found. Using system environment variables only.\")\n",
    "        \n",
    "        self._load_azure_config()\n",
    "        self._validate_config()\n",
    "    \n",
    "    def _load_azure_config(self):\n",
    "        \"\"\"Load Azure OpenAI configuration from environment variables\"\"\"\n",
    "        self.azure_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "        self.azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "        self.azure_deployment = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME', 'gpt-4o-mini')\n",
    "        self.azure_api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-05-01-preview')\n",
    "        self.tavily_api_key = os.getenv('TAVILY_API_KEY')\n",
    "    \n",
    "    def _validate_config(self):\n",
    "        \"\"\"Validate critical configuration\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        if not self.azure_api_key:\n",
    "            errors.append(\"AZURE_OPENAI_API_KEY is required\")\n",
    "        \n",
    "        if not self.azure_endpoint:\n",
    "            errors.append(\"AZURE_OPENAI_ENDPOINT is required\")\n",
    "            \n",
    "        if not self.tavily_api_key:\n",
    "            errors.append(\"TAVILY_API_KEY is required for search functionality\")\n",
    "        \n",
    "        if errors:\n",
    "            raise ValueError(f\"Configuration errors: {', '.join(errors)}\")\n",
    "        \n",
    "        print(\"‚úÖ All Azure OpenAI configuration validated successfully\")\n",
    "    \n",
    "    def display_config(self):\n",
    "        \"\"\"Display current configuration (hiding secrets)\"\"\"\n",
    "        print(\"Azure OpenAI Configuration:\")\n",
    "        print(f\"  Endpoint: {self.azure_endpoint}\")\n",
    "        print(f\"  Deployment: {self.azure_deployment}\")\n",
    "        print(f\"  API Version: {self.azure_api_version}\")\n",
    "        print(f\"  API Key: {'*' * 20 + self.azure_api_key[-4:] if self.azure_api_key else 'Not set'}\")\n",
    "        print(f\"  Tavily API Key: {'*' * 20 + self.tavily_api_key[-4:] if self.tavily_api_key else 'Not set'}\")\n",
    "\n",
    "# Initialize configuration\n",
    "try:\n",
    "    config = NotebookConfig()\n",
    "    config.display_config()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load configuration: {e}\")\n",
    "    print(\"\\nPlease ensure you have a .env file with the following variables:\")\n",
    "    print(\"- AZURE_OPENAI_API_KEY\")\n",
    "    print(\"- AZURE_OPENAI_ENDPOINT\")\n",
    "    print(\"- AZURE_OPENAI_CHAT_DEPLOYMENT_NAME (optional, defaults to gpt-4o-mini)\")\n",
    "    print(\"- TAVILY_API_KEY\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1183e",
   "metadata": {},
   "source": [
    "### 1.1 Basic Prompting and Model I/O with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe9ff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Let's break it down step by step:\n",
      "\n",
      "1. France is a country in Western Europe.\n",
      "2. Every country has a city that serves as its capital, where the government is based.\n",
      "3. The capital of France is a city known worldwide for its culture, history, and landmarks like the Eiffel Tower.\n",
      "4. That city is Paris.\n",
      "\n",
      "**Final Answer:** The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize Azure OpenAI with configuration\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_deployment,\n",
    "    api_version=config.azure_api_version,\n",
    "    temperature=0.7,\n",
    "    azure_endpoint=config.azure_endpoint,\n",
    "    api_key=config.azure_api_key\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Question: {question}\\nAnswer: Let's think step by step.\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca4b69",
   "metadata": {},
   "source": [
    "### 1.2 Sequential Processing with LCEL (Modern Pattern)\n",
    "\n",
    "LCEL (LangChain Expression Language) is the modern way to compose operations. Let's build a sequential workflow using the pipe operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38709b63",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (1366570620.py, line 48)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn RunnableParallel({\u001b[39m\n                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# Initialize Azure OpenAI with configuration\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_deployment,\n",
    "    api_version=config.azure_api_version,\n",
    "    temperature=0.7,\n",
    "    azure_endpoint=config.azure_endpoint,\n",
    "    api_key=config.azure_api_key\n",
    ")\n",
    "\n",
    "# Modern LCEL approach - compose operations with pipe operator\n",
    "name_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is a good name for a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "catchphrase_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a creative catchphrase for the following company: {company_name}\"\n",
    ")\n",
    "\n",
    "# Build sequential chain using LCEL composition\n",
    "name_chain = name_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Create a more complex chain that passes results between steps\n",
    "def create_sequential_chain():\n",
    "    \"\"\"Creates a sequential chain using modern LCEL patterns\"\"\"\n",
    "    \n",
    "    # Step 1: Generate company name\n",
    "    step1 = (\n",
    "        {\"product\": RunnablePassthrough()} \n",
    "        | name_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Step 2: Generate catchphrase using the company name\n",
    "    step2 = (\n",
    "        {\"company_name\": step1}\n",
    "        | catchphrase_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Combine results into final output\n",
    "    return RunnableParallel({\n",
    "        \"company_name\": step1,\n",
    "        \"catchphrase\": step2,\n",
    "        \"product\": RunnablePassthrough()\n",
    "    })\n",
    "\n",
    "# Execute the sequential chain\n",
    "sequential_chain = create_sequential_chain()\n",
    "product = \"colorful, eco-friendly socks\"\n",
    "\n",
    "print(f\"Input: {product}\")\n",
    "print(\"\\\\nüîó Running sequential LCEL chain...\")\n",
    "\n",
    "result = sequential_chain.invoke(product)\n",
    "print(f\"\\\\n‚úÖ Results:\")\n",
    "print(f\"Product: {result['product']}\")\n",
    "print(f\"Company Name: {result['company_name']}\")\n",
    "print(f\"Catchphrase: {result['catchphrase']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Streaming with LCEL\n",
    "\n",
    "One of LCEL's key advantages is built-in streaming support for real-time responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qxdfaptm2t",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class StreamingCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"Custom callback handler to demonstrate streaming\"\"\"\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "# Create a streaming chain\n",
    "streaming_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a brief story about {topic}. Make it engaging and creative.\"\n",
    ")\n",
    "\n",
    "streaming_chain = (\n",
    "    streaming_prompt \n",
    "    | llm.with_config({\"callbacks\": [StreamingCallbackHandler()]})\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"üåä Streaming response for a story about 'space exploration':\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Note: In a real notebook, you'd see the text appear token by token\n",
    "response = streaming_chain.invoke({\"topic\": \"space exploration\"})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ufwt2kl5sol",
   "metadata": {},
   "source": [
    "### 1.4 Function Calling and Basic Agents\n",
    "\n",
    "LangChain agents can use tools (functions) to interact with external systems. This demonstrates basic agent capabilities before we explore multi-agent systems in a dedicated notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODERN 2025 PATTERN: Using LangGraph instead of legacy AgentExecutor\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Initialize Azure OpenAI for agents (temperature=0 for more deterministic responses)\n",
    "llm_agent = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_deployment,\n",
    "    api_version=config.azure_api_version,\n",
    "    temperature=0,\n",
    "    azure_endpoint=config.azure_endpoint,\n",
    "    api_key=config.azure_api_key\n",
    ")\n",
    "\n",
    "# Define tools that the agent can use\n",
    "search_tool = TavilySearchResults(\n",
    "    max_results=2,\n",
    "    api_key=config.tavily_api_key,\n",
    "    description=\"Search the web for current information\"\n",
    ")\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"A simple calculator that evaluates basic mathematical expressions.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression like '2+2' or '4**0.5'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Basic safety check - only allow simple math operations\n",
    "        allowed_chars = set('0123456789+-*/().**')\n",
    "        if not all(c in allowed_chars or c.isspace() for c in expression):\n",
    "            return \"Error: Only basic mathematical operations are allowed\"\n",
    "        \n",
    "        result = eval(expression)\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "@tool \n",
    "def format_text(text: str, style: str = \"upper\") -> str:\n",
    "    \"\"\"Format text in different styles.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to format\n",
    "        style: The formatting style ('upper', 'lower', 'title')\n",
    "    \"\"\"\n",
    "    if style == \"upper\":\n",
    "        return text.upper()\n",
    "    elif style == \"lower\":\n",
    "        return text.lower()\n",
    "    elif style == \"title\":\n",
    "        return text.title()\n",
    "    else:\n",
    "        return f\"Unknown style: {style}. Use 'upper', 'lower', or 'title'\"\n",
    "\n",
    "# Create agent with tools using MODERN 2025 LangGraph pattern\n",
    "tools = [search_tool, calculate, format_text]\n",
    "\n",
    "# Modern approach: Create agent with memory using LangGraph\n",
    "system_message = \"You are a helpful assistant that can search the web, perform calculations, and format text. Always explain your reasoning step by step.\"\n",
    "\n",
    "# Initialize memory for conversation persistence\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Create the modern LangGraph agent (replaces legacy AgentExecutor)\n",
    "agent = create_react_agent(\n",
    "    model=llm_agent,\n",
    "    tools=tools,\n",
    "    prompt=system_message,\n",
    "    checkpointer=memory  # 2025 feature: built-in memory\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Testing MODERN 2025 LangGraph agent with multiple tool usage...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configuration for conversation threading (2025 memory feature)\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-conversation\"}}\n",
    "\n",
    "# Test multi-step reasoning with tool usage using modern streaming\n",
    "query = \"Calculate 16 raised to the power of 0.5, then format the result as 'The answer is X' in title case\"\n",
    "print(f\"üéØ Query: {query}\\n\")\n",
    "\n",
    "# MODERN 2025 PATTERN: Streaming execution with memory\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [(\"user\", query)]},\n",
    "    config=config,\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    if step:\n",
    "        print(f\"üìù Step: {step}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ MODERN 2025 FEATURES DEMONSTRATED:\")\n",
    "print(\"   üîÑ LangGraph agent (replaces legacy AgentExecutor)\")\n",
    "print(\"   üíæ Built-in memory with conversation threading\")\n",
    "print(\"   üåä Real-time streaming execution\")\n",
    "print(\"   üîç LangSmith tracing (if enabled)\")\n",
    "print(\"=\"* 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-integration",
   "metadata": {},
   "source": [
    "## üéØ 2025 Evaluation & Monitoring Integration\n",
    "\n",
    "Modern AI applications require comprehensive evaluation and monitoring. Let's integrate the leading 2025 frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025 PATTERN: Modern evaluation framework integration\n",
    "from langsmith import Client\n",
    "import os\n",
    "\n",
    "# Configure LangSmith for production monitoring\n",
    "if os.getenv('LANGCHAIN_API_KEY'):\n",
    "    langsmith_client = Client()\n",
    "    \n",
    "    # Enable tracing for the agent\n",
    "    os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "    os.environ['LANGCHAIN_PROJECT'] = 'LangChain-2025-Evaluation'\n",
    "    \n",
    "    print(\"‚úÖ LangSmith monitoring enabled\")\n",
    "    print(f\"üìä Project: {os.environ.get('LANGCHAIN_PROJECT')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LangSmith not configured (add LANGCHAIN_API_KEY to enable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deepeval-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025 PATTERN: DeepEval integration for comprehensive testing\n",
    "try:\n",
    "    from deepeval import evaluate\n",
    "    from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
    "    from deepeval.test_case import LLMTestCase\n",
    "    \n",
    "    # Create evaluation metrics\n",
    "    relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "    faithfulness_metric = FaithfulnessMetric(threshold=0.7)\n",
    "    \n",
    "    # Example test case\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"What are the main benefits of LangGraph?\",\n",
    "        actual_output=\"LangGraph provides state management, complex agent workflows, and streaming capabilities for multi-agent systems.\",\n",
    "        expected_output=\"LangGraph offers state management and multi-agent orchestration capabilities.\",\n",
    "        context=[\"LangGraph is LangChain's framework for building stateful, multi-agent applications\"]\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluation_results = evaluate([test_case], [relevancy_metric, faithfulness_metric])\n",
    "    print(\"‚úÖ DeepEval integration working\")\n",
    "    print(f\"üìä Evaluation results: {evaluation_results}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è DeepEval not installed (pip install deepeval to enable advanced evaluation)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è DeepEval evaluation error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost-tracking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025 PATTERN: Cost tracking and token usage monitoring\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def run_agent_with_cost_tracking(agent, query):\n",
    "    \"\"\"Run agent with comprehensive cost and performance tracking\"\"\"\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        # Configure conversation with thread ID for tracking\n",
    "        config = {\"configurable\": {\"thread_id\": \"cost-tracking-demo\"}}\n",
    "        \n",
    "        # Run the agent with streaming\n",
    "        response = None\n",
    "        for chunk in agent.stream({\"messages\": [(\"human\", query)]}, config):\n",
    "            if \"agent\" in chunk:\n",
    "                response = chunk[\"agent\"][\"messages\"][-1].content\n",
    "    \n",
    "    # Display cost metrics\n",
    "    print(\"\\nüí∞ Cost Analysis:\")\n",
    "    print(f\"  Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"  Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"  Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"  Total Cost: ${cb.total_cost:.4f}\")\n",
    "    \n",
    "    return response, {\n",
    "        'total_tokens': cb.total_tokens,\n",
    "        'cost': cb.total_cost,\n",
    "        'prompt_tokens': cb.prompt_tokens,\n",
    "        'completion_tokens': cb.completion_tokens\n",
    "    }\n",
    "\n",
    "# Example usage with cost tracking\n",
    "if 'agent' in locals():\n",
    "    test_query = \"What's the current weather in San Francisco?\"\n",
    "    response, metrics = run_agent_with_cost_tracking(agent, test_query)\n",
    "    \n",
    "    print(f\"\\nü§ñ Agent Response: {response}\")\n",
    "    print(f\"üìä Performance Metrics: {metrics}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Agent not initialized - run the previous cells first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y3bla5ukyb9",
   "metadata": {},
   "source": [
    "## Summary: LangChain Fundamentals (2025 Edition)\n",
    "\n",
    "This notebook covered the core concepts of LangChain using modern patterns:\n",
    "\n",
    "### Key Concepts Learned:\n",
    "1. **LCEL (LangChain Expression Language)**: Modern composition using pipe operators\n",
    "2. **Sequential Processing**: Chaining operations with result passing between steps\n",
    "3. **Streaming**: Real-time token streaming for better user experience  \n",
    "4. **LangGraph Agents**: Modern agent framework replacing legacy AgentExecutor\n",
    "5. **Evaluation Integration**: LangSmith monitoring and DeepEval testing frameworks\n",
    "6. **Cost Tracking**: Production-ready token usage and cost monitoring\n",
    "\n",
    "### 2025 LangChain Evolution:\n",
    "- **LangGraph**: State management and multi-agent orchestration\n",
    "- **LangSmith**: Production monitoring and evaluation workflows  \n",
    "- **Modern Agent Patterns**: Memory-enabled agents with conversation threading\n",
    "- **Evaluation Frameworks**: Comprehensive testing with DeepEval integration\n",
    "- **Production Monitoring**: Built-in cost tracking and performance metrics\n",
    "\n",
    "### Next Steps:\n",
    "- **Multi-Agent Systems**: See `3_langchain_agents_langgraph.ipynb` for complex workflows\n",
    "- **Production Deployment**: LangSmith tracing and monitoring setup\n",
    "- **Advanced Evaluation**: Custom metrics and automated testing pipelines\n",
    "- **Cost Optimization**: Token usage analysis and efficiency improvements\n",
    "\n",
    "### 2025 Production Considerations:\n",
    "- Enable LangSmith tracing for production monitoring\n",
    "- Implement DeepEval for comprehensive agent testing\n",
    "- Use conversation threading for memory-enabled applications\n",
    "- Monitor costs with built-in callback handlers\n",
    "- Leverage LangGraph for complex multi-agent orchestration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
