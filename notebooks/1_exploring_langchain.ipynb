{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 1: Hands-On with LangChain (2025 Edition)\n\nThis notebook demonstrates LangChain's **2025 capabilities** using Azure OpenAI. We'll explore modern LCEL patterns, LangGraph agents (replacing legacy AgentExecutor), LangSmith integration, and evaluation frameworks.\n\n## üÜï What's New in 2025:\n- **LangGraph** replaces legacy AgentExecutor patterns\n- **LangSmith** production-ready monitoring and evaluation  \n- **LangChain Sandbox** for safe code execution\n- **Modern evaluation frameworks** integration",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's load our environment variables and configure our Azure OpenAI connection with **LangSmith tracing** for 2025 observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and configuration setup with 2025 LangSmith integration\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "class NotebookConfig:\n",
    "    \"\"\"Configuration management for Azure OpenAI and LangSmith in Jupyter notebooks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load environment variables\n",
    "        env_path = find_dotenv()\n",
    "        if env_path:\n",
    "            load_dotenv(env_path)\n",
    "            print(f\"‚úÖ Loaded environment from: {env_path}\")\n",
    "        else:\n",
    "            warnings.warn(\"No .env file found. Using system environment variables only.\")\n",
    "        \n",
    "        self._load_azure_config()\n",
    "        self._validate_config()\n",
    "    \n",
    "    def _load_azure_config(self):\n",
    "        \"\"\"Load Azure OpenAI configuration from environment variables\"\"\"\n",
    "        self.azure_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "        self.azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "        self.azure_deployment = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME', 'gpt-4o-mini')\n",
    "        self.azure_api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-05-01-preview')\n",
    "        self.tavily_api_key = os.getenv('TAVILY_API_KEY')\n",
    "    \n",
    "    def _validate_config(self):\n",
    "        \"\"\"Validate critical configuration\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        if not self.azure_api_key:\n",
    "            errors.append(\"AZURE_OPENAI_API_KEY is required\")\n",
    "        \n",
    "        if not self.azure_endpoint:\n",
    "            errors.append(\"AZURE_OPENAI_ENDPOINT is required\")\n",
    "            \n",
    "        if not self.tavily_api_key:\n",
    "            errors.append(\"TAVILY_API_KEY is required for search functionality\")\n",
    "        \n",
    "        if errors:\n",
    "            raise ValueError(f\"Configuration errors: {', '.join(errors)}\")\n",
    "        \n",
    "        print(\"‚úÖ All Azure OpenAI configuration validated successfully\")\n",
    "    \n",
    "    def display_config(self):\n",
    "        \"\"\"Display current configuration (hiding secrets)\"\"\"\n",
    "        print(\"Azure OpenAI Configuration:\")\n",
    "        print(f\"  Endpoint: {self.azure_endpoint}\")\n",
    "        print(f\"  Deployment: {self.azure_deployment}\")\n",
    "        print(f\"  API Version: {self.azure_api_version}\")\n",
    "        print(f\"  API Key: {'*' * 20 + self.azure_api_key[-4:] if self.azure_api_key else 'Not set'}\")\n",
    "        print(f\"  Tavily API Key: {'*' * 20 + self.tavily_api_key[-4:] if self.tavily_api_key else 'Not set'}\")\n",
    "\n",
    "# Initialize configuration\n",
    "try:\n",
    "    config = NotebookConfig()\n",
    "    config.display_config()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load configuration: {e}\")\n",
    "    print(\"\\nPlease ensure you have a .env file with the following variables:\")\n",
    "    print(\"- AZURE_OPENAI_API_KEY\")\n",
    "    print(\"- AZURE_OPENAI_ENDPOINT\")\n",
    "    print(\"- AZURE_OPENAI_CHAT_DEPLOYMENT_NAME (optional, defaults to gpt-4o-mini)\")\n",
    "    print(\"- TAVILY_API_KEY\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1 Basic Prompting and Model I/O with LCEL",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize Azure OpenAI with configuration\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_deployment,\n",
    "    api_version=config.azure_api_version,\n",
    "    temperature=0.7,\n",
    "    azure_endpoint=config.azure_endpoint,\n",
    "    api_key=config.azure_api_key\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Question: {question}\\nAnswer: Let's think step by step.\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 Sequential Processing with LCEL (Modern Pattern)\n\nLCEL (LangChain Expression Language) is the modern way to compose operations. Let's build a sequential workflow using the pipe operator.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import AzureChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Initialize Azure OpenAI with configuration\nllm = AzureChatOpenAI(\n    azure_deployment=config.azure_deployment,\n    api_version=config.azure_api_version,\n    temperature=0.7,\n    azure_endpoint=config.azure_endpoint,\n    api_key=config.azure_api_key\n)\n\n# Modern LCEL approach - compose operations with pipe operator\nname_prompt = ChatPromptTemplate.from_template(\n    \"What is a good name for a company that makes {product}?\"\n)\n\ncatchphrase_prompt = ChatPromptTemplate.from_template(\n    \"Write a creative catchphrase for the following company: {company_name}\"\n)\n\n# Build sequential chain using LCEL composition\nname_chain = name_prompt | llm | StrOutputParser()\n\n# Create a more complex chain that passes results between steps\ndef create_sequential_chain():\n    \"\"\"Creates a sequential chain using modern LCEL patterns\"\"\"\n    \n    # Step 1: Generate company name\n    step1 = (\n        {\"product\": RunnablePassthrough()} \n        | name_prompt \n        | llm \n        | StrOutputParser()\n    )\n    \n    # Step 2: Generate catchphrase using the company name\n    step2 = (\n        {\"company_name\": step1}\n        | catchphrase_prompt\n        | llm\n        | StrOutputParser()\n    )\n    \n    # Combine results into final output\n    return {\n        \"company_name\": step1,\n        \"catchphrase\": step2,\n        \"product\": RunnablePassthrough()\n    }\n\n# Execute the sequential chain\nsequential_chain = create_sequential_chain()\nproduct = \"colorful, eco-friendly socks\"\n\nprint(f\"Input: {product}\")\nprint(\"\\\\nüîó Running sequential LCEL chain...\")\n\nresult = sequential_chain.invoke(product)\nprint(f\"\\\\n‚úÖ Results:\")\nprint(f\"Product: {result['product']}\")\nprint(f\"Company Name: {result['company_name']}\")\nprint(f\"Catchphrase: {result['catchphrase']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 Streaming with LCEL\n\nOne of LCEL's key advantages is built-in streaming support for real-time responses.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qxdfaptm2t",
   "source": "import time\nfrom langchain_core.callbacks import BaseCallbackHandler\n\nclass StreamingCallbackHandler(BaseCallbackHandler):\n    \"\"\"Custom callback handler to demonstrate streaming\"\"\"\n    \n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        print(token, end=\"\", flush=True)\n\n# Create a streaming chain\nstreaming_prompt = ChatPromptTemplate.from_template(\n    \"Write a brief story about {topic}. Make it engaging and creative.\"\n)\n\nstreaming_chain = (\n    streaming_prompt \n    | llm.with_config({\"callbacks\": [StreamingCallbackHandler()]})\n    | StrOutputParser()\n)\n\nprint(\"üåä Streaming response for a story about 'space exploration':\")\nprint(\"=\" * 50)\n\n# Note: In a real notebook, you'd see the text appear token by token\nresponse = streaming_chain.invoke({\"topic\": \"space exploration\"})\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"‚úÖ Streaming complete!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ufwt2kl5sol",
   "source": "### 1.4 Function Calling and Basic Agents\n\nLangChain agents can use tools (functions) to interact with external systems. This demonstrates basic agent capabilities before we explore multi-agent systems in a dedicated notebook.",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# MODERN 2025 PATTERN: Using LangGraph instead of legacy AgentExecutor\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain.tools import tool\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# Initialize Azure OpenAI for agents (temperature=0 for more deterministic responses)\nllm_agent = AzureChatOpenAI(\n    azure_deployment=config.azure_deployment,\n    api_version=config.azure_api_version,\n    temperature=0,\n    azure_endpoint=config.azure_endpoint,\n    api_key=config.azure_api_key\n)\n\n# Define tools that the agent can use\nsearch_tool = TavilySearchResults(\n    max_results=2,\n    api_key=config.tavily_api_key,\n    description=\"Search the web for current information\"\n)\n\n@tool\ndef calculate(expression: str) -> str:\n    \"\"\"A simple calculator that evaluates basic mathematical expressions.\n    \n    Args:\n        expression: A mathematical expression like '2+2' or '4**0.5'\n    \"\"\"\n    try:\n        # Basic safety check - only allow simple math operations\n        allowed_chars = set('0123456789+-*/().**')\n        if not all(c in allowed_chars or c.isspace() for c in expression):\n            return \"Error: Only basic mathematical operations are allowed\"\n        \n        result = eval(expression)\n        return f\"Result: {result}\"\n    except Exception as e:\n        return f\"Error evaluating expression: {e}\"\n\n@tool \ndef format_text(text: str, style: str = \"upper\") -> str:\n    \"\"\"Format text in different styles.\n    \n    Args:\n        text: The text to format\n        style: The formatting style ('upper', 'lower', 'title')\n    \"\"\"\n    if style == \"upper\":\n        return text.upper()\n    elif style == \"lower\":\n        return text.lower()\n    elif style == \"title\":\n        return text.title()\n    else:\n        return f\"Unknown style: {style}. Use 'upper', 'lower', or 'title'\"\n\n# Create agent with tools using MODERN 2025 LangGraph pattern\ntools = [search_tool, calculate, format_text]\n\n# Modern approach: Create agent with memory using LangGraph\nsystem_message = \"You are a helpful assistant that can search the web, perform calculations, and format text. Always explain your reasoning step by step.\"\n\n# Initialize memory for conversation persistence\nmemory = MemorySaver()\n\n# Create the modern LangGraph agent (replaces legacy AgentExecutor)\nagent = create_react_agent(\n    model=llm_agent,\n    tools=tools,\n    prompt=system_message,\n    checkpointer=memory  # 2025 feature: built-in memory\n)\n\nprint(\"ü§ñ Testing MODERN 2025 LangGraph agent with multiple tool usage...\")\nprint(\"=\" * 50)\n\n# Configuration for conversation threading (2025 memory feature)\nconfig = {\"configurable\": {\"thread_id\": \"demo-conversation\"}}\n\n# Test multi-step reasoning with tool usage using modern streaming\nquery = \"Calculate 16 raised to the power of 0.5, then format the result as 'The answer is X' in title case\"\nprint(f\"üéØ Query: {query}\\n\")\n\n# MODERN 2025 PATTERN: Streaming execution with memory\nfor step in agent.stream(\n    {\"messages\": [(\"user\", query)]},\n    config=config,\n    stream_mode=\"updates\"\n):\n    if step:\n        print(f\"üìù Step: {step}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"‚úÖ MODERN 2025 FEATURES DEMONSTRATED:\")\nprint(\"   üîÑ LangGraph agent (replaces legacy AgentExecutor)\")\nprint(\"   üíæ Built-in memory with conversation threading\")\nprint(\"   üåä Real-time streaming execution\")\nprint(\"   üîç LangSmith tracing (if enabled)\")\nprint(\"=\"* 50)"
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-integration",
   "metadata": {},
   "source": "## üéØ 2025 Evaluation & Monitoring Integration\n\nModern AI applications require comprehensive evaluation and monitoring. Let's integrate the leading 2025 frameworks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-setup",
   "metadata": {},
   "outputs": [],
   "source": "# 2025 PATTERN: Modern evaluation framework integration\nfrom langsmith import Client\nimport os\n\n# Configure LangSmith for production monitoring\nif os.getenv('LANGCHAIN_API_KEY'):\n    langsmith_client = Client()\n    \n    # Enable tracing for the agent\n    os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n    os.environ['LANGCHAIN_PROJECT'] = 'LangChain-2025-Evaluation'\n    \n    print(\"‚úÖ LangSmith monitoring enabled\")\n    print(f\"üìä Project: {os.environ.get('LANGCHAIN_PROJECT')}\")\nelse:\n    print(\"‚ö†Ô∏è LangSmith not configured (add LANGCHAIN_API_KEY to enable)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deepeval-integration", 
   "metadata": {},
   "outputs": [],
   "source": "# 2025 PATTERN: DeepEval integration for comprehensive testing\ntry:\n    from deepeval import evaluate\n    from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n    from deepeval.test_case import LLMTestCase\n    \n    # Create evaluation metrics\n    relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n    faithfulness_metric = FaithfulnessMetric(threshold=0.7)\n    \n    # Example test case\n    test_case = LLMTestCase(\n        input=\"What are the main benefits of LangGraph?\",\n        actual_output=\"LangGraph provides state management, complex agent workflows, and streaming capabilities for multi-agent systems.\",\n        expected_output=\"LangGraph offers state management and multi-agent orchestration capabilities.\",\n        context=[\"LangGraph is LangChain's framework for building stateful, multi-agent applications\"]\n    )\n    \n    # Run evaluation\n    evaluation_results = evaluate([test_case], [relevancy_metric, faithfulness_metric])\n    print(\"‚úÖ DeepEval integration working\")\n    print(f\"üìä Evaluation results: {evaluation_results}\")\n    \nexcept ImportError:\n    print(\"‚ö†Ô∏è DeepEval not installed (pip install deepeval to enable advanced evaluation)\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è DeepEval evaluation error: {str(e)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost-tracking",
   "metadata": {},
   "outputs": [],
   "source": "# 2025 PATTERN: Cost tracking and token usage monitoring\nfrom langchain.callbacks import get_openai_callback\n\ndef run_agent_with_cost_tracking(agent, query):\n    \"\"\"Run agent with comprehensive cost and performance tracking\"\"\"\n    \n    with get_openai_callback() as cb:\n        # Configure conversation with thread ID for tracking\n        config = {\"configurable\": {\"thread_id\": \"cost-tracking-demo\"}}\n        \n        # Run the agent with streaming\n        response = None\n        for chunk in agent.stream({\"messages\": [(\"human\", query)]}, config):\n            if \"agent\" in chunk:\n                response = chunk[\"agent\"][\"messages\"][-1].content\n    \n    # Display cost metrics\n    print(\"\\nüí∞ Cost Analysis:\")\n    print(f\"  Total Tokens: {cb.total_tokens}\")\n    print(f\"  Prompt Tokens: {cb.prompt_tokens}\")\n    print(f\"  Completion Tokens: {cb.completion_tokens}\")\n    print(f\"  Total Cost: ${cb.total_cost:.4f}\")\n    \n    return response, {\n        'total_tokens': cb.total_tokens,\n        'cost': cb.total_cost,\n        'prompt_tokens': cb.prompt_tokens,\n        'completion_tokens': cb.completion_tokens\n    }\n\n# Example usage with cost tracking\nif 'agent' in locals():\n    test_query = \"What's the current weather in San Francisco?\"\n    response, metrics = run_agent_with_cost_tracking(agent, test_query)\n    \n    print(f\"\\nü§ñ Agent Response: {response}\")\n    print(f\"üìä Performance Metrics: {metrics}\")\nelse:\n    print(\"‚ö†Ô∏è Agent not initialized - run the previous cells first\")"
  },
  {
   "cell_type": "markdown",
   "id": "y3bla5ukyb9",
   "source": "## Summary: LangChain Fundamentals (2025 Edition)\n\nThis notebook covered the core concepts of LangChain using modern patterns:\n\n### Key Concepts Learned:\n1. **LCEL (LangChain Expression Language)**: Modern composition using pipe operators\n2. **Sequential Processing**: Chaining operations with result passing between steps\n3. **Streaming**: Real-time token streaming for better user experience  \n4. **LangGraph Agents**: Modern agent framework replacing legacy AgentExecutor\n5. **Evaluation Integration**: LangSmith monitoring and DeepEval testing frameworks\n6. **Cost Tracking**: Production-ready token usage and cost monitoring\n\n### 2025 LangChain Evolution:\n- **LangGraph**: State management and multi-agent orchestration\n- **LangSmith**: Production monitoring and evaluation workflows  \n- **Modern Agent Patterns**: Memory-enabled agents with conversation threading\n- **Evaluation Frameworks**: Comprehensive testing with DeepEval integration\n- **Production Monitoring**: Built-in cost tracking and performance metrics\n\n### Next Steps:\n- **Multi-Agent Systems**: See `3_langchain_agents_langgraph.ipynb` for complex workflows\n- **Production Deployment**: LangSmith tracing and monitoring setup\n- **Advanced Evaluation**: Custom metrics and automated testing pipelines\n- **Cost Optimization**: Token usage analysis and efficiency improvements\n\n### 2025 Production Considerations:\n- Enable LangSmith tracing for production monitoring\n- Implement DeepEval for comprehensive agent testing\n- Use conversation threading for memory-enabled applications\n- Monitor costs with built-in callback handlers\n- Leverage LangGraph for complex multi-agent orchestration",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}