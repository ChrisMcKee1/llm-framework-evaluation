{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Hands-On with LangChain\n",
    "\n",
    "This notebook demonstrates LangChain's core capabilities using Azure OpenAI. We'll explore basic prompting, sequential chains, and agent-based tool usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's load our environment variables and configure our Azure OpenAI connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and configuration setup\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "class NotebookConfig:\n",
    "    \"\"\"Configuration management for Azure OpenAI in Jupyter notebooks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load environment variables\n",
    "        env_path = find_dotenv()\n",
    "        if env_path:\n",
    "            load_dotenv(env_path)\n",
    "            print(f\"✅ Loaded environment from: {env_path}\")\n",
    "        else:\n",
    "            warnings.warn(\"No .env file found. Using system environment variables only.\")\n",
    "        \n",
    "        self._load_azure_config()\n",
    "        self._validate_config()\n",
    "    \n",
    "    def _load_azure_config(self):\n",
    "        \"\"\"Load Azure OpenAI configuration from environment variables\"\"\"\n",
    "        self.azure_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "        self.azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "        self.azure_deployment = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME', 'gpt-4o-mini')\n",
    "        self.azure_api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-05-01-preview')\n",
    "        self.tavily_api_key = os.getenv('TAVILY_API_KEY')\n",
    "    \n",
    "    def _validate_config(self):\n",
    "        \"\"\"Validate critical configuration\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        if not self.azure_api_key:\n",
    "            errors.append(\"AZURE_OPENAI_API_KEY is required\")\n",
    "        \n",
    "        if not self.azure_endpoint:\n",
    "            errors.append(\"AZURE_OPENAI_ENDPOINT is required\")\n",
    "            \n",
    "        if not self.tavily_api_key:\n",
    "            errors.append(\"TAVILY_API_KEY is required for search functionality\")\n",
    "        \n",
    "        if errors:\n",
    "            raise ValueError(f\"Configuration errors: {', '.join(errors)}\")\n",
    "        \n",
    "        print(\"✅ All Azure OpenAI configuration validated successfully\")\n",
    "    \n",
    "    def display_config(self):\n",
    "        \"\"\"Display current configuration (hiding secrets)\"\"\"\n",
    "        print(\"Azure OpenAI Configuration:\")\n",
    "        print(f\"  Endpoint: {self.azure_endpoint}\")\n",
    "        print(f\"  Deployment: {self.azure_deployment}\")\n",
    "        print(f\"  API Version: {self.azure_api_version}\")\n",
    "        print(f\"  API Key: {'*' * 20 + self.azure_api_key[-4:] if self.azure_api_key else 'Not set'}\")\n",
    "        print(f\"  Tavily API Key: {'*' * 20 + self.tavily_api_key[-4:] if self.tavily_api_key else 'Not set'}\")\n",
    "\n",
    "# Initialize configuration\n",
    "try:\n",
    "    config = NotebookConfig()\n",
    "    config.display_config()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load configuration: {e}\")\n",
    "    print(\"\\nPlease ensure you have a .env file with the following variables:\")\n",
    "    print(\"- AZURE_OPENAI_API_KEY\")\n",
    "    print(\"- AZURE_OPENAI_ENDPOINT\")\n",
    "    print(\"- AZURE_OPENAI_CHAT_DEPLOYMENT_NAME (optional, defaults to gpt-4o-mini)\")\n",
    "    print(\"- TAVILY_API_KEY\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1 Basic Prompting and Model I/O with LCEL",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize Azure OpenAI with configuration\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_deployment,\n",
    "    api_version=config.azure_api_version,\n",
    "    temperature=0.7,\n",
    "    azure_endpoint=config.azure_endpoint,\n",
    "    api_key=config.azure_api_key\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Question: {question}\\nAnswer: Let's think step by step.\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 Sequential Processing with LCEL (Modern Pattern)\n\nLCEL (LangChain Expression Language) is the modern way to compose operations. Let's build a sequential workflow using the pipe operator.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import AzureChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Initialize Azure OpenAI with configuration\nllm = AzureChatOpenAI(\n    azure_deployment=config.azure_deployment,\n    api_version=config.azure_api_version,\n    temperature=0.7,\n    azure_endpoint=config.azure_endpoint,\n    api_key=config.azure_api_key\n)\n\n# Modern LCEL approach - compose operations with pipe operator\nname_prompt = ChatPromptTemplate.from_template(\n    \"What is a good name for a company that makes {product}?\"\n)\n\ncatchphrase_prompt = ChatPromptTemplate.from_template(\n    \"Write a creative catchphrase for the following company: {company_name}\"\n)\n\n# Build sequential chain using LCEL composition\nname_chain = name_prompt | llm | StrOutputParser()\n\n# Create a more complex chain that passes results between steps\ndef create_sequential_chain():\n    \"\"\"Creates a sequential chain using modern LCEL patterns\"\"\"\n    \n    # Step 1: Generate company name\n    step1 = (\n        {\"product\": RunnablePassthrough()} \n        | name_prompt \n        | llm \n        | StrOutputParser()\n    )\n    \n    # Step 2: Generate catchphrase using the company name\n    step2 = (\n        {\"company_name\": step1}\n        | catchphrase_prompt\n        | llm\n        | StrOutputParser()\n    )\n    \n    # Combine results into final output\n    return {\n        \"company_name\": step1,\n        \"catchphrase\": step2,\n        \"product\": RunnablePassthrough()\n    }\n\n# Execute the sequential chain\nsequential_chain = create_sequential_chain()\nproduct = \"colorful, eco-friendly socks\"\n\nprint(f\"Input: {product}\")\nprint(\"\\\\n🔗 Running sequential LCEL chain...\")\n\nresult = sequential_chain.invoke(product)\nprint(f\"\\\\n✅ Results:\")\nprint(f\"Product: {result['product']}\")\nprint(f\"Company Name: {result['company_name']}\")\nprint(f\"Catchphrase: {result['catchphrase']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 Streaming with LCEL\n\nOne of LCEL's key advantages is built-in streaming support for real-time responses.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qxdfaptm2t",
   "source": "import time\nfrom langchain_core.callbacks import BaseCallbackHandler\n\nclass StreamingCallbackHandler(BaseCallbackHandler):\n    \"\"\"Custom callback handler to demonstrate streaming\"\"\"\n    \n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        print(token, end=\"\", flush=True)\n\n# Create a streaming chain\nstreaming_prompt = ChatPromptTemplate.from_template(\n    \"Write a brief story about {topic}. Make it engaging and creative.\"\n)\n\nstreaming_chain = (\n    streaming_prompt \n    | llm.with_config({\"callbacks\": [StreamingCallbackHandler()]})\n    | StrOutputParser()\n)\n\nprint(\"🌊 Streaming response for a story about 'space exploration':\")\nprint(\"=\" * 50)\n\n# Note: In a real notebook, you'd see the text appear token by token\nresponse = streaming_chain.invoke({\"topic\": \"space exploration\"})\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"✅ Streaming complete!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ufwt2kl5sol",
   "source": "### 1.4 Function Calling and Basic Agents\n\nLangChain agents can use tools (functions) to interact with external systems. This demonstrates basic agent capabilities before we explore multi-agent systems in a dedicated notebook.",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain.tools import tool\nfrom langchain import hub\nfrom langchain.agents import create_openai_functions_agent, AgentExecutor\n\n# Initialize Azure OpenAI for agents (temperature=0 for more deterministic responses)\nllm_agent = AzureChatOpenAI(\n    azure_deployment=config.azure_deployment,\n    api_version=config.azure_api_version,\n    temperature=0,\n    azure_endpoint=config.azure_endpoint,\n    api_key=config.azure_api_key\n)\n\n# Define tools that the agent can use\nsearch_tool = TavilySearchResults(\n    max_results=2,\n    api_key=config.tavily_api_key,\n    description=\"Search the web for current information\"\n)\n\n@tool\ndef calculate(expression: str) -> str:\n    \"\"\"A simple calculator that evaluates basic mathematical expressions.\n    \n    Args:\n        expression: A mathematical expression like '2+2' or '4**0.5'\n    \"\"\"\n    try:\n        # Basic safety check - only allow simple math operations\n        allowed_chars = set('0123456789+-*/().**')\n        if not all(c in allowed_chars or c.isspace() for c in expression):\n            return \"Error: Only basic mathematical operations are allowed\"\n        \n        result = eval(expression)\n        return f\"Result: {result}\"\n    except Exception as e:\n        return f\"Error evaluating expression: {e}\"\n\n@tool \ndef format_text(text: str, style: str = \"upper\") -> str:\n    \"\"\"Format text in different styles.\n    \n    Args:\n        text: The text to format\n        style: The formatting style ('upper', 'lower', 'title')\n    \"\"\"\n    if style == \"upper\":\n        return text.upper()\n    elif style == \"lower\":\n        return text.lower()\n    elif style == \"title\":\n        return text.title()\n    else:\n        return f\"Unknown style: {style}. Use 'upper', 'lower', or 'title'\"\n\n# Create agent with tools\ntools = [search_tool, calculate, format_text]\n\n# Pull a pre-built prompt for function calling agents\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\nagent = create_openai_functions_agent(llm_agent, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=3)\n\nprint(\"🤖 Testing agent with multiple tool usage...\")\nprint(\"=\" * 50)\n\n# Test multi-step reasoning with tool usage\nresponse = agent_executor.invoke({\n    \"input\": \"Calculate 16 raised to the power of 0.5, then format the result as 'The answer is X' in title case\"\n})\n\nprint(f\"\\\\n✅ Final Response: {response['output']}\")\nprint(\"=\" * 50)\nprint(\"\\\\n📝 Note: This demonstrates single-agent tool usage. For complex multi-agent workflows, see the LangGraph notebook.\")"
  },
  {
   "cell_type": "markdown",
   "id": "y3bla5ukyb9",
   "source": "## Summary: LangChain Fundamentals\n\nThis notebook covered the core concepts of LangChain using modern patterns:\n\n### Key Concepts Learned:\n1. **LCEL (LangChain Expression Language)**: Modern composition using pipe operators\n2. **Sequential Processing**: Chaining operations with result passing between steps\n3. **Streaming**: Real-time token streaming for better user experience  \n4. **Function Calling**: Single agents using tools for external capabilities\n\n### LangChain's Philosophy:\n- **\"Compose Everything\"**: Build complex workflows by composing simple components\n- **LCEL as Core**: Unified interface for chaining, streaming, and async operations\n- **Tool Integration**: Rich ecosystem of 700+ integrations available\n\n### Next Steps:\n- **Multi-Agent Systems**: See `3_langchain_agents_langgraph.ipynb` for complex agent workflows\n- **Production Patterns**: Error handling, retry logic, and monitoring\n- **Advanced LCEL**: Complex routing, fallbacks, and parallel execution\n\n### Production Considerations:\n- Always use environment variables for API keys\n- Implement proper error handling and timeout management\n- Add logging and monitoring for production deployments\n- Consider token usage and cost optimization strategies",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}