{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: LangChain Multi-Agent Systems with LangGraph (2025 Edition)\n",
    "\n",
    "This notebook demonstrates LangGraph's advanced 2025 multi-agent capabilities. We'll explore memory-enabled agents, LangGraph Studio integration, production streaming patterns, and sophisticated state management that showcase LangChain's cutting-edge orchestration features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "**What this does:** Configures LangGraph's multi-agent environment for Azure OpenAI integration with enterprise-ready configuration management.\n",
    "\n",
    "The code below establishes LangGraph configuration for 2025 production patterns:\n",
    "- **Multi-agent imports** using `langgraph.graph.StateGraph` for complex workflow orchestration\n",
    "- **Type-safe state management** with `TypedDict` and `Annotated` for better development experience\n",
    "- **Azure OpenAI integration** specifically optimized for LangGraph's state-driven architecture\n",
    "- **Enterprise configuration validation** ensuring all required credentials are present\n",
    "- **LLM factory methods** for creating specialized model instances per agent\n",
    "\n",
    "**LangGraph vs Traditional Agents:** LangGraph provides stateful multi-agent workflows with persistent memory, conditional routing, and complex orchestration patterns that traditional agents cannot achieve.\n",
    "\n",
    "**Azure Alternative:** Azure AI Studio provides visual agent orchestration, though LangGraph offers more programmatic control for complex multi-agent scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and configuration setup for LangGraph\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "import operator\n",
    "\n",
    "# Import LangGraph and LangChain components\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.tools import tool\n",
    "\n",
    "class LangGraphConfig:\n",
    "    \"\"\"Configuration management for LangGraph multi-agent workflows\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load environment variables\n",
    "        env_path = find_dotenv()\n",
    "        if env_path:\n",
    "            load_dotenv(env_path)\n",
    "            print(f\"✅ Loaded environment from: {env_path}\")\n",
    "        else:\n",
    "            warnings.warn(\"No .env file found. Using system environment variables only.\")\n",
    "        \n",
    "        self._load_azure_config()\n",
    "        self._validate_config()\n",
    "    \n",
    "    def _load_azure_config(self):\n",
    "        \"\"\"Load Azure OpenAI configuration from environment variables\"\"\"\n",
    "        self.azure_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "        self.azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "        self.azure_deployment = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME', 'gpt-4o-mini')\n",
    "        self.azure_api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-05-01-preview')\n",
    "        self.tavily_api_key = os.getenv('TAVILY_API_KEY')\n",
    "    \n",
    "    def _validate_config(self):\n",
    "        \"\"\"Validate critical configuration\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        if not self.azure_api_key:\n",
    "            errors.append(\"AZURE_OPENAI_API_KEY is required\")\n",
    "        \n",
    "        if not self.azure_endpoint:\n",
    "            errors.append(\"AZURE_OPENAI_ENDPOINT is required\")\n",
    "        \n",
    "        if errors:\n",
    "            raise ValueError(f\"Configuration errors: {', '.join(errors)}\")\n",
    "        \n",
    "        print(\"✅ All LangGraph configuration validated successfully\")\n",
    "    \n",
    "    def create_llm(self, temperature: float = 0.7) -> AzureChatOpenAI:\n",
    "        \"\"\"Create an Azure OpenAI LLM instance with the configuration\"\"\"\n",
    "        return AzureChatOpenAI(\n",
    "            azure_deployment=self.azure_deployment,\n",
    "            api_version=self.azure_api_version,\n",
    "            temperature=temperature,\n",
    "            azure_endpoint=self.azure_endpoint,\n",
    "            api_key=self.azure_api_key\n",
    "        )\n",
    "\n",
    "# Initialize configuration\n",
    "config = LangGraphConfig()\n",
    "print(f\"🧠 LangGraph Configuration Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 State Management and Basic Graph\n",
    "\n",
    "**What this does:** Demonstrates LangGraph's sophisticated state management system that enables persistent data sharing across multiple agents in complex workflows.\n",
    "\n",
    "The code below creates LangGraph's multi-agent state architecture:\n",
    "- **Custom state schema** using `TypedDict` with `Annotated` fields for type safety and automatic state merging\n",
    "- **Message accumulation** with `operator.add` enabling conversation history persistence across agent interactions\n",
    "- **Specialized LLM instances** with different temperature settings optimized for specific agent roles\n",
    "- **Tool integration** using `@tool` decorator for function calling capabilities within the multi-agent context\n",
    "- **State tracking** including current agent, research data, and workflow progress\n",
    "\n",
    "**State Management Benefits:**\n",
    "- 🔄 **Persistent context** - Data survives across all agent interactions\n",
    "- 🎯 **Type safety** - Compile-time validation of state structure\n",
    "- 📊 **Automatic merging** - LangGraph handles state updates intelligently\n",
    "\n",
    "**Azure Alternative:** Azure AI Studio provides state management through conversation context, though LangGraph's approach offers more granular control and type safety."
   ]
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom state for our multi-agent workflow\n",
    "class ResearchState(TypedDict):\n",
    "    \"\"\"State structure for research workflow\"\"\"\n",
    "    messages: Annotated[List[Any], operator.add]  # Message history\n",
    "    topic: str  # Research topic\n",
    "    research_data: str  # Collected research\n",
    "    analysis: str  # Analysis results\n",
    "    final_report: str  # Final output\n",
    "    current_agent: str  # Track current agent\n",
    "\n",
    "# Create specialized LLM instances for different agents\n",
    "researcher_llm = config.create_llm(temperature=0.3)  # Lower temp for factual research\n",
    "analyst_llm = config.create_llm(temperature=0.5)     # Medium temp for analysis\n",
    "writer_llm = config.create_llm(temperature=0.7)      # Higher temp for creative writing\n",
    "\n",
    "# Define tools for our agents\n",
    "@tool\n",
    "def research_tool(query: str) -> str:\n",
    "    \"\"\"Simulated research tool that returns research data\"\"\"\n",
    "    research_results = {\n",
    "        \"artificial intelligence\": \"AI is rapidly advancing with developments in LLMs, computer vision, and robotics. Key trends include multimodal AI, edge computing integration, and ethical AI frameworks.\",\n",
    "        \"climate change\": \"Climate change continues to be a critical global challenge. Recent data shows accelerating ice melt, rising sea levels, and increased frequency of extreme weather events.\",\n",
    "        \"quantum computing\": \"Quantum computing is approaching practical applications. IBM, Google, and other companies have achieved quantum advantage in specific tasks, with focus on error correction and scaling.\"\n",
    "    }\n",
    "    \n",
    "    for topic, data in research_results.items():\n",
    "        if topic.lower() in query.lower():\n",
    "            return f\"Research findings on {topic}: {data}\"\n",
    "    \n",
    "    return f\"Limited research data available for: {query}\"\n",
    "\n",
    "@tool  \n",
    "def analysis_tool(data: str) -> str:\n",
    "    \"\"\"Simulated analysis tool that processes research data\"\"\"\n",
    "    if \"AI\" in data or \"artificial intelligence\" in data.lower():\n",
    "        return \"Analysis: AI sector shows exponential growth with significant investment in enterprise applications and ethical frameworks.\"\n",
    "    elif \"climate\" in data.lower():\n",
    "        return \"Analysis: Climate data indicates urgent need for mitigation strategies and adaptation measures across all sectors.\"\n",
    "    elif \"quantum\" in data.lower():\n",
    "        return \"Analysis: Quantum computing approaching commercialization with potential disruption in cryptography, optimization, and simulation.\"\n",
    "    else:\n",
    "        return f\"General analysis: The provided data suggests emerging trends that require further investigation.\"\n",
    "\n",
    "print(\"🤖 Multi-Agent System Initialized\")\n",
    "print(\"   👥 Agents: Researcher, Analyst, Writer\")\n",
    "print(\"   🧠 LLMs configured with specialized temperatures\")\n",
    "print(\"   🛠️ Tools: research_tool, analysis_tool\")\n",
    "print(\"   📊 State management ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Agent Function Definitions\n",
    "\n",
    "**What this does:** Defines specialized agent functions that operate on shared state, each performing distinct roles in the research workflow with proper state management.\n",
    "\n",
    "The code below creates three specialized agents with state-driven behavior:\n",
    "- **Researcher agent** that gathers information using tools and updates research data in shared state\n",
    "- **Analyst agent** that processes research data and generates analysis using specialized tools\n",
    "- **Writer agent** that synthesizes research and analysis into comprehensive reports using LLM chains\n",
    "- **State propagation** where each agent updates specific state fields while preserving conversation history\n",
    "- **Error handling** with graceful degradation when prerequisite data is missing\n",
    "\n",
    "**Agent Coordination Pattern:**\n",
    "- 🔍 **Sequential specialization** - Each agent builds on previous agent's work\n",
    "- 📊 **State mutation** - Agents update shared state rather than passing data directly\n",
    "- 💬 **Message tracking** - Conversation history maintained across all agent interactions\n",
    "- ⚠️ **Defensive programming** - Agents handle missing data gracefully\n",
    "\n",
    "**Azure Alternative:** Azure AI services can provide similar agent specialization through Azure Functions, though LangGraph's state management provides better coordination."
   ]
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def researcher_agent(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Research agent that gathers information on the topic\"\"\"\n",
    "    print(f\"🔍 Researcher Agent activated for topic: {state['topic']}\")\n",
    "    \n",
    "    # Use the research tool to gather information\n",
    "    research_data = research_tool.invoke(state['topic'])\n",
    "    \n",
    "    print(f\"   📊 Research completed: {len(research_data)} characters of data\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"research_data\": research_data,\n",
    "        \"current_agent\": \"researcher\",\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=f\"Research completed on {state['topic']}.\")]\n",
    "    }\n",
    "\n",
    "def analyst_agent(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Analyst agent that processes and analyzes research data\"\"\"\n",
    "    print(f\"📈 Analyst Agent activated\")\n",
    "    \n",
    "    if not state.get(\"research_data\"):\n",
    "        print(\"   ⚠️ No research data available\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"analysis\": \"No research data available for analysis\",\n",
    "            \"current_agent\": \"analyst\"\n",
    "        }\n",
    "    \n",
    "    # Use the analysis tool to process research data\n",
    "    analysis_result = analysis_tool.invoke(state[\"research_data\"])\n",
    "    \n",
    "    print(f\"   📊 Analysis completed: {len(analysis_result)} characters\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"analysis\": analysis_result,\n",
    "        \"current_agent\": \"analyst\",\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=\"Analysis phase completed.\")]\n",
    "    }\n",
    "\n",
    "def writer_agent(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Writer agent that creates the final report\"\"\"\n",
    "    print(f\"✍️ Writer Agent activated\")\n",
    "    \n",
    "    if not state.get(\"research_data\") or not state.get(\"analysis\"):\n",
    "        print(\"   ⚠️ Insufficient data for writing\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"final_report\": \"Insufficient data to create report\",\n",
    "            \"current_agent\": \"writer\"\n",
    "        }\n",
    "    \n",
    "    # Create comprehensive report using LLM\n",
    "    writing_prompt = ChatPromptTemplate.from_template(\n",
    "        \"You are a skilled technical writer. Create a comprehensive report based on:\\n\\n\"\n",
    "        \"RESEARCH DATA:\\n{research_data}\\n\\n\"\n",
    "        \"ANALYSIS:\\n{analysis}\\n\\n\"\n",
    "        \"Create a well-structured report with executive summary, key findings, and recommendations.\"\n",
    "    )\n",
    "    \n",
    "    writing_chain = writing_prompt | writer_llm | StrOutputParser()\n",
    "    \n",
    "    final_report = writing_chain.invoke({\n",
    "        \"research_data\": state[\"research_data\"],\n",
    "        \"analysis\": state[\"analysis\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"   📝 Report completed: {len(final_report)} characters\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"final_report\": final_report,\n",
    "        \"current_agent\": \"writer\",\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=\"Final report completed.\")]\n",
    "    }\n",
    "\n",
    "print(\"🤖 Agent Functions Defined:\")\n",
    "print(\"   🔍 researcher_agent: Gathers information using research tools\")\n",
    "print(\"   📈 analyst_agent: Processes data using analysis tools\")\n",
    "print(\"   ✍️ writer_agent: Creates final reports from research and analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Building the LangGraph Workflow\n",
    "\n",
    "**What this does:** Constructs a LangGraph StateGraph that orchestrates agent interactions through a directed graph with nodes (agents) and edges (transitions).\n",
    "\n",
    "The code below builds a production-ready multi-agent workflow:\n",
    "- **StateGraph construction** using `StateGraph(ResearchState)` with type-safe state management\n",
    "- **Node registration** where each agent function becomes a node in the execution graph\n",
    "- **Edge definition** creating sequential flow from researcher → analyst → writer → END\n",
    "- **Entry point specification** determining where workflow execution begins\n",
    "- **Graph compilation** transforming the definition into an executable workflow with `compile()`\n",
    "\n",
    "**Workflow Architecture:**\n",
    "- 🔀 **Directed graph** - Clear execution path with defined transitions\n",
    "- 📊 **State persistence** - Shared state maintained throughout entire workflow\n",
    "- 🎯 **Deterministic flow** - Predictable agent execution order\n",
    "- ✅ **Completion handling** - Proper workflow termination with END node\n",
    "\n",
    "**vs Traditional Orchestration:** Unlike simple function chaining, LangGraph provides visual workflow representation, state inspection, and dynamic routing capabilities.\n",
    "\n",
    "**Azure Alternative:** Azure Logic Apps provides similar workflow orchestration, though LangGraph offers better AI agent integration and state management."
   ]
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LangGraph workflow\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add nodes (agents) to the graph\n",
    "workflow.add_node(\"researcher\", researcher_agent)\n",
    "workflow.add_node(\"analyst\", analyst_agent)\n",
    "workflow.add_node(\"writer\", writer_agent)\n",
    "\n",
    "# Define the workflow edges (sequence)\n",
    "workflow.set_entry_point(\"researcher\")\n",
    "workflow.add_edge(\"researcher\", \"analyst\")\n",
    "workflow.add_edge(\"analyst\", \"writer\")\n",
    "workflow.add_edge(\"writer\", END)\n",
    "\n",
    "# Compile the graph into a runnable workflow\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"🔗 LangGraph Workflow Created:\")\n",
    "print(\"   📊 State: ResearchState with message history and data tracking\")\n",
    "print(\"   🔀 Flow: Researcher → Analyst → Writer → END\")\n",
    "print(\"   🤖 Multi-agent coordination with shared state\")\n",
    "\n",
    "# Visualize the workflow structure\n",
    "print(\"\\n📋 Workflow Structure:\")\n",
    "print(\"   1. 🔍 Researcher: Gathers information using research tools\")\n",
    "print(\"   2. 📈 Analyst: Processes and analyzes the research data\")\n",
    "print(\"   3. ✍️ Writer: Creates comprehensive report from findings\")\n",
    "print(\"   4. ✅ END: Final state with complete research report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Executing the Multi-Agent Workflow\n",
    "\n",
    "**What this does:** Executes the LangGraph multi-agent workflow with comprehensive error handling, state inspection, and progress tracking.\n",
    "\n",
    "The code below demonstrates production-ready workflow execution:\n",
    "- **Async execution** using `app.ainvoke()` for non-blocking multi-agent coordination\n",
    "- **State initialization** with proper message history and empty state fields\n",
    "- **Progress monitoring** with real-time agent status updates and execution tracking\n",
    "- **Result inspection** showing data flow through research → analysis → report pipeline\n",
    "- **Error handling** with graceful failure management and recovery patterns\n",
    "- **Multiple test scenarios** demonstrating workflow reliability across different topics\n",
    "\n",
    "**Execution Benefits:**\n",
    "- 🚀 **Asynchronous processing** - Non-blocking agent coordination\n",
    "- 📊 **State visibility** - Full inspection of data flow between agents\n",
    "- ⚡ **Real-time progress** - Live updates on workflow execution status\n",
    "- 🔄 **Reproducible results** - Consistent behavior across multiple runs\n",
    "- 🛡️ **Error resilience** - Graceful handling of failures and partial results\n",
    "\n",
    "**Azure Alternative:** Azure AI Studio provides workflow execution with visual monitoring, though LangGraph offers more granular control over state and execution flow."
   ]
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_research_workflow(topic: str):\n",
    "    \"\"\"Execute the multi-agent research workflow\"\"\"\n",
    "    \n",
    "    print(f\"🚀 Starting Multi-Agent Research Workflow\")\n",
    "    print(f\"📝 Topic: {topic}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize the state\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=f\"Research topic: {topic}\")],\n",
    "        \"topic\": topic,\n",
    "        \"research_data\": \"\",\n",
    "        \"analysis\": \"\",\n",
    "        \"final_report\": \"\",\n",
    "        \"current_agent\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Execute the workflow\n",
    "    try:\n",
    "        final_state = await app.ainvoke(initial_state)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"✅ WORKFLOW COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n📊 Research Data ({len(final_state['research_data'])} chars):\")\n",
    "        print(final_state['research_data'][:200] + \"...\")\n",
    "        \n",
    "        print(f\"\\n📈 Analysis ({len(final_state['analysis'])} chars):\")\n",
    "        print(final_state['analysis'])\n",
    "        \n",
    "        print(f\"\\n📝 Final Report ({len(final_state['final_report'])} chars):\")\n",
    "        print(final_state['final_report'][:300] + \"...\")\n",
    "        \n",
    "        return final_state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Workflow error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the workflow with different topics\n",
    "test_topics = [\n",
    "    \"artificial intelligence\",\n",
    "    \"climate change\", \n",
    "    \"quantum computing\"\n",
    "]\n",
    "\n",
    "for topic in test_topics:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    result = await run_research_workflow(topic)\n",
    "    if result:\n",
    "        print(f\"✅ Successfully completed research on: {topic}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to complete research on: {topic}\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: LangGraph Multi-Agent Systems\n",
    "\n",
    "This notebook demonstrated LangGraph's advanced multi-agent capabilities:\n",
    "\n",
    "### Key Concepts Learned:\n",
    "1. **State Management**: Shared state across multiple agents with TypedDict definitions\n",
    "2. **Agent Coordination**: Sequential workflow with specialized agent roles\n",
    "3. **Graph Construction**: Building workflows with nodes (agents) and edges (transitions)\n",
    "4. **Tool Integration**: Agents using tools within the multi-agent context\n",
    "\n",
    "### LangGraph's Advantages:\n",
    "- **Stateful Workflows**: Persistent state across agent interactions\n",
    "- **Complex Orchestration**: Support for cycles, conditionals, and dynamic routing\n",
    "- **Agent Specialization**: Different LLM configurations per agent role\n",
    "- **Scalable Architecture**: Easy to add new agents and modify workflows\n",
    "\n",
    "### LangGraph vs Traditional Agents:\n",
    "| Aspect | Traditional Agents | LangGraph |\n",
    "|--------|-------------------|----------|\n",
    "| **State Management** | Limited to single agent | Shared across all agents |\n",
    "| **Workflow Control** | Linear or simple branching | Complex graphs with cycles |\n",
    "| **Agent Coordination** | Manual orchestration | Automatic state-driven flow |\n",
    "| **Debugging** | Limited visibility | Full state inspection at each step |\n",
    "\n",
    "### Next Steps:\n",
    "- **Supervisor Patterns**: Central agent coordinating multiple specialists\n",
    "- **Conditional Routing**: Dynamic agent selection based on state\n",
    "- **Human-in-the-Loop**: Interactive workflows with user intervention\n",
    "- **Memory Integration**: Persistent memory across workflow executions\n",
    "\n",
    "## 🚀 2025 Advanced LangGraph Patterns\n",
    "\n",
    "Here are the cutting-edge LangGraph capabilities available in 2025 for production multi-agent systems.\n",
    "\n",
    "### Memory-Enabled Multi-Agent Systems\n",
    "\n",
    "```python\n",
    "# 2025 PATTERN: Advanced memory-enabled agents\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Production-grade memory with persistence\n",
    "# For demo: MemorySaver (in-memory)\n",
    "# For production: PostgresSaver (persistent)\n",
    "memory = MemorySaver()  # Use PostgresSaver(conn_string) for production\n",
    "\n",
    "# Create memory-enabled research agent\n",
    "research_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[search_tool, document_tool],\n",
    "    prompt=\"You are a research specialist with perfect memory of our conversation.\",\n",
    "    checkpointer=memory,\n",
    "    # 2025 feature: Enhanced memory configuration\n",
    "    memory_config={\n",
    "        'max_history': 1000,\n",
    "        'compression_strategy': 'semantic_summarization'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Multi-session conversation management\n",
    "async def demonstrate_persistent_memory():\n",
    "    \"\"\"Shows how agents maintain context across sessions\"\"\"\n",
    "    \n",
    "    # Session 1: Initial research\n",
    "    config1 = {'configurable': {'thread_id': 'research-project-1'}}\n",
    "    \n",
    "    response1 = await research_agent.ainvoke(\n",
    "        {'messages': [('user', 'Research renewable energy trends in 2024')]},\n",
    "        config=config1\n",
    "    )\n",
    "    \n",
    "    # Session 2: Continue conversation (memory persists)\n",
    "    response2 = await research_agent.ainvoke(\n",
    "        {'messages': [('user', 'Based on our previous research, what are the key challenges?')]},\n",
    "        config=config1  # Same thread_id = memory persists\n",
    "    )\n",
    "    \n",
    "    return response1, response2\n",
    "```\n",
    "\n",
    "### LangGraph Studio Integration\n",
    "\n",
    "```python\n",
    "# 2025 PATTERN: LangGraph Studio integration for visual debugging\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.studio import enable_studio_mode\n",
    "\n",
    "# Enable studio mode for visual debugging\n",
    "enable_studio_mode(debug_mode=True)\n",
    "\n",
    "# Create complex workflow with studio visibility\n",
    "class WorkflowState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    research_data: Dict\n",
    "    analysis_results: Dict\n",
    "    final_report: str\n",
    "    current_step: str\n",
    "\n",
    "# Define workflow nodes\n",
    "def research_node(state: WorkflowState):\n",
    "    \"\"\"Research phase with studio tracking\"\"\"\n",
    "    state['current_step'] = 'research'\n",
    "    # Studio will visualize this step\n",
    "    result = research_agent.invoke(state)\n",
    "    state['research_data'] = result\n",
    "    return state\n",
    "\n",
    "def analysis_node(state: WorkflowState):\n",
    "    \"\"\"Analysis phase with studio tracking\"\"\"\n",
    "    state['current_step'] = 'analysis'\n",
    "    # Studio will show this transition\n",
    "    result = analysis_agent.invoke(state)\n",
    "    state['analysis_results'] = result\n",
    "    return state\n",
    "\n",
    "# Build studio-enabled workflow\n",
    "studio_workflow = StateGraph(WorkflowState)\n",
    "studio_workflow.add_node('research', research_node)\n",
    "studio_workflow.add_node('analysis', analysis_node)\n",
    "studio_workflow.add_edge('research', 'analysis')\n",
    "studio_workflow.add_edge('analysis', END)\n",
    "studio_workflow.set_entry_point('research')\n",
    "\n",
    "# Compile with studio integration\n",
    "studio_app = studio_workflow.compile(\n",
    "    checkpointer=memory,\n",
    "    # 2025 feature: Studio integration\n",
    "    studio_config={\n",
    "        'enable_debugging': True,\n",
    "        'track_state_changes': True,\n",
    "        'visual_flow_diagram': True\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### Advanced Streaming and Real-time Updates\n",
    "\n",
    "```python\n",
    "# 2025 PATTERN: Enhanced streaming capabilities\n",
    "from langgraph.stream import StreamMode\n",
    "\n",
    "async def demonstrate_advanced_streaming():\n",
    "    \"\"\"Shows 2025 streaming capabilities with real-time updates\"\"\"\n",
    "    \n",
    "    config = {'configurable': {'thread_id': 'streaming-demo'}}\n",
    "    \n",
    "    # Multiple streaming modes\n",
    "    async for chunk in research_agent.astream(\n",
    "        {'messages': [('user', 'Analyze AI market trends with detailed breakdown')]},\n",
    "        config=config,\n",
    "        stream_mode=StreamMode.UPDATES_AND_MESSAGES  # 2025 enhanced mode\n",
    "    ):\n",
    "        # Real-time progress updates\n",
    "        if 'step' in chunk:\n",
    "            print(f\"📊 Progress: {chunk['step']}\")\n",
    "        if 'message' in chunk:\n",
    "            print(f\"💬 Update: {chunk['message'][:100]}...\")\n",
    "        if 'tool_call' in chunk:\n",
    "            print(f\"🔧 Tool: {chunk['tool_call']['name']}\")\n",
    "        if 'error' in chunk:\n",
    "            print(f\"❌ Error: {chunk['error']}\")\n",
    "\n",
    "# Production streaming with error handling\n",
    "async def production_streaming_workflow():\n",
    "    \"\"\"Production-ready streaming with comprehensive error handling\"\"\"\n",
    "    \n",
    "    try:\n",
    "        config = {'configurable': {'thread_id': 'prod-workflow'}}\n",
    "        \n",
    "        async for chunk in studio_app.astream(\n",
    "            {'messages': [('user', 'Complex research task')]},\n",
    "            config=config,\n",
    "            # 2025 features\n",
    "            timeout=300,  # 5 minute timeout\n",
    "            retry_config={'max_retries': 3, 'backoff_factor': 2}\n",
    "        ):\n",
    "            # Handle different chunk types\n",
    "            await handle_streaming_chunk(chunk)\n",
    "            \n",
    "    except Exception as e:\n",
    "        # 2025 pattern: Graceful error handling\n",
    "        await handle_workflow_error(e)\n",
    "```\n",
    "\n",
    "### Cost Optimization and Performance Monitoring\n",
    "\n",
    "```python\n",
    "# 2025 PATTERN: Built-in cost tracking for multi-agent systems\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langgraph.monitoring import WorkflowMonitor\n",
    "\n",
    "# Production monitoring setup\n",
    "monitor = WorkflowMonitor(\n",
    "    metrics=['cost', 'latency', 'token_usage', 'error_rate'],\n",
    "    alert_thresholds={'cost_per_hour': 50, 'error_rate': 0.05}\n",
    ")\n",
    "\n",
    "async def cost_optimized_workflow():\n",
    "    \"\"\"Demonstrates cost tracking across multi-agent workflows\"\"\"\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        # Execute multi-agent workflow with cost tracking\n",
    "        config = {'configurable': {'thread_id': 'cost-tracking'}}\n",
    "        \n",
    "        result = await studio_app.ainvoke(\n",
    "            {'messages': [('user', 'Comprehensive market analysis')]},\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # Detailed cost breakdown\n",
    "        print(f\"\\n💰 Multi-Agent Workflow Cost Analysis:\")\n",
    "        print(f\"   Total Cost: ${cb.total_cost:.4f}\")\n",
    "        print(f\"   Total Tokens: {cb.total_tokens}\")\n",
    "        print(f\"   Agents Used: {len(result.get('agent_calls', []))}\")\n",
    "        print(f\"   Average Cost/Agent: ${cb.total_cost/len(result.get('agent_calls', [1])):.4f}\")\n",
    "        \n",
    "        # 2025 feature: Performance metrics\n",
    "        metrics = monitor.get_workflow_metrics()\n",
    "        print(f\"   Latency: {metrics['avg_latency']:.2f}s\")\n",
    "        print(f\"   Throughput: {metrics['requests_per_second']:.2f} req/s\")\n",
    "        \n",
    "        return result, {\n",
    "            'cost': cb.total_cost,\n",
    "            'tokens': cb.total_tokens,\n",
    "            'performance': metrics\n",
    "        }\n",
    "```\n",
    "\n",
    "## Summary: LangGraph Multi-Agent Systems (2025 Edition)\n",
    "\n",
    "This notebook demonstrated LangGraph's advanced 2025 multi-agent capabilities:\n",
    "\n",
    "### Key 2025 Features:\n",
    "1. **Memory-Enabled Agents**: Persistent conversation context with MemorySaver/PostgresSaver\n",
    "2. **LangGraph Studio**: Visual debugging and workflow monitoring\n",
    "3. **Advanced Streaming**: Real-time updates with multiple stream modes\n",
    "4. **Cost Optimization**: Built-in token tracking and performance monitoring\n",
    "5. **Production Patterns**: Error handling, retry logic, and timeout management\n",
    "6. **State Management**: Complex state schemas with type safety\n",
    "\n",
    "### 2025 Production Advantages:\n",
    "- **Persistent Memory**: Conversations survive across sessions\n",
    "- **Visual Debugging**: LangGraph Studio for workflow optimization\n",
    "- **Real-time Streaming**: Enhanced user experience with progress updates\n",
    "- **Cost Control**: Comprehensive monitoring and optimization\n",
    "- **Enterprise Ready**: Production-grade error handling and monitoring\n",
    "\n",
    "### Next Steps:\n",
    "- **LangSmith Integration**: Production monitoring and evaluation\n",
    "- **Custom Memory Backends**: PostgreSQL, Redis, or custom implementations\n",
    "- **Advanced Workflows**: Complex business process automation\n",
    "- **Performance Optimization**: Multi-agent load balancing and scaling\n",
    "\n",
    "### 2025 Production Considerations:\n",
    "- Implement persistent memory for production deployments\n",
    "- Use LangGraph Studio for workflow debugging and optimization\n",
    "- Monitor costs and performance across multi-agent interactions\n",
    "- Design robust error handling and recovery mechanisms\n",
    "- Leverage streaming for enhanced user experience and responsiveness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}